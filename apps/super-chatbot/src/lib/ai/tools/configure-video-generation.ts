import { tool } from "ai";
import { z } from "zod";
import type { MediaOption } from "@/lib/types/media-settings";
import { getVideoGenerationConfig } from "@/lib/config/media-settings-factory";
import {
  checkBalanceBeforeArtifact,
  getOperationDisplayName,
} from "@/lib/utils/ai-tools-balance";
import type { Session } from "next-auth";
import { analyzeVideoContext } from "@/lib/ai/context";

interface CreateVideoDocumentParams {
  createDocument: any;
  session?: Session | null;
  defaultSourceVideoUrl?: string;
  defaultSourceImageUrl?: string;
  chatId?: string;
  userMessage?: string;
  currentAttachments?: any[];
}

export const configureVideoGeneration = (params?: CreateVideoDocumentParams) =>
  tool({
    description:
      "Configure video generation settings or generate a video directly if prompt is provided. Supports text-to-video by default, video-to-video when a video sourceVideoUrl is provided, and image-to-video when an image sourceVideoUrl is provided. When triggered, creates a video artifact that shows generation progress in real-time.",
    parameters: z.object({
      prompt: z
        .string()
        .optional()
        .describe(
          "Detailed description of the video to generate. If provided, will immediately create video artifact and start generation"
        ),
      sourceVideoUrl: z
        .string()
        .url()
        .optional()
        .describe(
          "Optional source URL for video generation. Can be a video URL for video-to-video generation, or an image URL for image-to-video generation (e.g., when the user uploaded media in chat). If provided, the system will run the appropriate generation type."
        ),
      style: z
        .string()
        .optional()
        .describe(
          'Style of the video. Supports many formats: "realistic", "cinematic", "anime", "cartoon", "documentary", "vlog", "tutorial", "promotional", "artistic", "minimalist", "abstract", and many more available styles'
        ),
      resolution: z
        .string()
        .optional()
        .describe(
          'Video resolution. Accepts various formats: "1920x1080", "1920Ã—1080", "1920 x 1080", "full hd", "fhd", "1080p", "4k", "square", "vertical", "horizontal", etc.'
        ),
      duration: z
        .string()
        .optional()
        .describe(
          'Video duration. Accepts: "5s", "10s", "30s", "1m", "2m", "short", "medium", "long", etc.'
        ),
      model: z
        .string()
        .optional()
        .describe(
          'AI model to use. Models are loaded dynamically from SuperDuperAI API. Use model name like "VEO3" or full model ID.'
        ),
      seed: z.number().optional().describe("Seed for reproducible results"),
      batchSize: z
        .number()
        .min(1)
        .max(2)
        .optional()
        .describe(
          "Number of videos to generate simultaneously (1-2). Higher batch sizes generate multiple variations at once."
        ),
    }),
    execute: async ({
      prompt,
      sourceVideoUrl,
      style,
      resolution,
      duration,
      model,
      seed,
      batchSize,
    }) => {
      console.log("ðŸŽ¬ configureVideoGeneration called with:", {
        prompt,
        style,
        resolution,
        duration,
        model,
        seed,
        batchSize,
      });

      // AICODE-NOTE: Use new factory to get configuration with OpenAPI models
      console.log("ðŸŽ¬ Loading video configuration from OpenAPI factory...");
      const config = await getVideoGenerationConfig();

      console.log("ðŸŽ¬ âœ… Loaded video config:", {
        modelsCount: config.availableModels.length,
        resolutionsCount: config.availableResolutions.length,
        stylesCount: config.availableStyles.length,
      });

      // If no prompt provided, return configuration panel
      if (!prompt) {
        console.log(
          "ðŸŽ¬ No prompt provided, returning video configuration panel"
        );
        return config;
      }

      console.log("ðŸŽ¬ âœ… PROMPT PROVIDED, CREATING VIDEO DOCUMENT:", prompt);

      if (!params?.createDocument) {
        console.log(
          "ðŸŽ¬ âŒ createDocument not available, returning basic config"
        );
        return config;
      }

      // Check style for quality multipliers
      const multipliers: string[] = [];
      if (style?.includes("high-quality")) multipliers.push("high-quality");
      if (style?.includes("ultra-quality")) multipliers.push("ultra-quality");

      try {
        // Find the selected options or use defaults from factory
        const selectedResolution = resolution
          ? config.availableResolutions.find((r) => r.label === resolution) ||
            config.defaultSettings.resolution
          : config.defaultSettings.resolution;

        let selectedStyle: MediaOption = config.defaultSettings.style;
        if (style) {
          const foundStyle = findVideoStyle(style, config.availableStyles);
          if (foundStyle) {
            selectedStyle = foundStyle;
            console.log(
              "ðŸŽ¬ âœ… STYLE MATCHED:",
              style,
              "->",
              selectedStyle.label
            );
          } else {
            console.log(
              "ðŸŽ¬ âš ï¸ STYLE NOT FOUND:",
              style,
              "using default:",
              selectedStyle.label
            );
          }
        }

        const selectedDuration = duration
          ? config.availableDurations.find(
              (d) => d.label === duration || d.id === duration
            ) || config.defaultSettings.duration
          : config.defaultSettings.duration;

        const selectedModel = model
          ? config.availableModels.find(
              (m) => m.name === model || (m as any).id === model
            ) || config.defaultSettings.model
          : config.defaultSettings.model;

        // Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        let normalizedSourceUrl = sourceVideoUrl;

        console.log("ðŸ” configureVideoGeneration sourceVideoUrl resolution:", {
          sourceVideoUrl,
          defaultSourceVideoUrl: params?.defaultSourceVideoUrl,
          chatId: params?.chatId,
          userMessage: params?.userMessage,
        });

        // ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ 1: Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° (ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº)
        if (params?.chatId && params?.userMessage) {
          try {
            console.log("ðŸ” Analyzing video context with new system...");
            const contextResult = await analyzeVideoContext(
              params.userMessage,
              params.chatId,
              params.currentAttachments
            );

            console.log("ðŸ” Context analysis result:", contextResult);

            if (contextResult.sourceUrl && contextResult.confidence !== "low") {
              console.log(
                "ðŸ” Using sourceUrl from new context analysis:",
                contextResult.sourceUrl,
                "confidence:",
                contextResult.confidence
              );
              normalizedSourceUrl = contextResult.sourceUrl;
            }
          } catch (error) {
            console.warn("ðŸ” Error in context analysis, falling back:", error);
          }
        }

        // ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ 2: defaultSourceVideoUrl (legacy Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°) - Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð½Ðµ Ð´Ð°Ð» Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
        if (
          !normalizedSourceUrl &&
          params?.defaultSourceVideoUrl &&
          /^https?:\/\//.test(params.defaultSourceVideoUrl)
        ) {
          console.log(
            "ðŸ” Using defaultSourceVideoUrl from legacy context analysis:",
            params.defaultSourceVideoUrl
          );
          normalizedSourceUrl = params.defaultSourceVideoUrl;
        }

        // ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ 3: AI-provided sourceVideoUrl
        if (
          !normalizedSourceUrl &&
          sourceVideoUrl &&
          /^https?:\/\//.test(sourceVideoUrl) &&
          !sourceVideoUrl.startsWith("attachment://")
        ) {
          console.log("ðŸ” Using AI-provided sourceVideoUrl:", sourceVideoUrl);
          normalizedSourceUrl = sourceVideoUrl;
        }
        // Fallback: text-to-video
        if (!normalizedSourceUrl) {
          console.log(
            "ðŸ” No valid source video URL available, will be text-to-video"
          );
        }

        // Determine operation type and check balance
        let operationType = "text-to-video";
        if (normalizedSourceUrl) {
          // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð»Ð¸ Ð²Ð¸Ð´ÐµÐ¾
          const isImageSource =
            /\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i.test(normalizedSourceUrl) ||
            normalizedSourceUrl.includes("image/") ||
            params?.currentAttachments?.some(
              (att) =>
                att.url === normalizedSourceUrl &&
                String(att.contentType || "").startsWith("image/")
            );

          operationType = isImageSource ? "image-to-video" : "video-to-video";

          console.log("ðŸ” Operation type determined:", {
            sourceUrl: normalizedSourceUrl,
            isImageSource,
            operationType,
          });
        }

        // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð±Ñ‹Ð» Ð»Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð´Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
        if (
          params?.userMessage &&
          normalizedSourceUrl &&
          operationType === "image-to-video"
        ) {
          // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð»Ð¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð¼Ñƒ
          const semanticSearchPatterns = [
            /(ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº[Ð°-Ñ]+\s+Ñ\s+|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ\s+Ñ\s+|Ñ„Ð¾Ñ‚Ð¾\s+Ñ\s+|image\s+with\s+|picture\s+with\s+|photo\s+with\s+)/i,
            /(ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ðº[Ð°-Ñ]+\s+Ð³Ð´Ðµ\s+ÐµÑÑ‚ÑŒ|Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ\s+Ð³Ð´Ðµ\s+ÐµÑÑ‚ÑŒ|Ñ„Ð¾Ñ‚Ð¾\s+Ð³Ð´Ðµ\s+ÐµÑÑ‚ÑŒ|image\s+that\s+has|picture\s+that\s+contains|photo\s+that\s+shows)/i,
          ];

          const hasSemanticSearchRequest = semanticSearchPatterns.some(
            (pattern) => pattern.test(params.userMessage || "")
          );

          if (hasSemanticSearchRequest) {
            // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð±Ñ‹Ð» Ð»Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ‡ÐµÑ€ÐµÐ· ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº
            // Ð•ÑÐ»Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð±Ñ‹Ð» Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‡ÐµÑ€ÐµÐ· fallback (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ), ÑÑ‚Ð¾ Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»
            const isFallbackSource =
              params.defaultSourceVideoUrl === normalizedSourceUrl ||
              params.defaultSourceImageUrl === normalizedSourceUrl;

            console.log("ðŸ” Fallback check:", {
              normalizedSourceUrl,
              defaultSourceVideoUrl: params.defaultSourceVideoUrl,
              defaultSourceImageUrl: params.defaultSourceImageUrl,
              isFallbackSource,
              hasSemanticSearchRequest,
            });

            if (isFallbackSource) {
              console.log(
                "ðŸ” Semantic search failed, providing helpful message instead of balance error"
              );
              return {
                error: "semantic_search_failed",
                message:
                  "Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Ñ Ð½Ðµ ÑÐ¼Ð¾Ð³ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ Ð½ÑƒÐ¶Ð½Ñ‹Ð¼ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ñ‹Ð¼ Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ:\n\nâ€¢ Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ Ð½ÑƒÐ¶Ð½Ñ‹Ð¼ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ñ‹Ð¼\nâ€¢ ÐžÐ¿Ð¸ÑÐ°Ñ‚ÑŒ ÑÑ†ÐµÐ½Ñƒ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\nâ€¢ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÐµÐµ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'Ð¿ÐµÑ€Ð²Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ' Ð¸Ð»Ð¸ 'Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ')",
                suggestions: [
                  "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ Ð»ÑƒÐ½Ð¾Ð¹",
                  "Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ñ Ð»ÑƒÐ½Ð¾Ð¹",
                  "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ",
                  "ÐžÐ¿Ð¸ÑÐ°Ñ‚ÑŒ ÑÑ†ÐµÐ½Ñƒ Ð´Ð»Ñ Ð°Ð½Ð¸Ð¼Ð°Ñ†Ð¸Ð¸",
                ],
              };
            }
          }
        }

        const balanceCheck = await checkBalanceBeforeArtifact(
          params.session || null,
          "video-generation",
          operationType,
          multipliers,
          getOperationDisplayName(operationType)
        );

        if (!balanceCheck.valid) {
          console.log("ðŸŽ¬ âŒ INSUFFICIENT BALANCE, NOT CREATING ARTIFACT");
          return {
            error:
              balanceCheck.userMessage ||
              "ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ ÑÑ€ÐµÐ´ÑÑ‚Ð² Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾",
            balanceError: true,
            requiredCredits: balanceCheck.cost,
          };
        }

        // Create the video document with all parameters
        const videoParams = {
          prompt,
          style: selectedStyle,
          resolution: selectedResolution,
          duration: selectedDuration.value || selectedDuration, // Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ value Ð´Ð»Ñ API
          model: selectedModel,
          seed: seed || undefined,
          batchSize: batchSize || 1,
          ...(normalizedSourceUrl
            ? { sourceVideoUrl: normalizedSourceUrl }
            : {}),
        };

        console.log("ðŸŽ¬ âœ… CREATING VIDEO DOCUMENT WITH PARAMS:", videoParams);
        console.log("ðŸ” Final sourceVideoUrl used:", normalizedSourceUrl);

        try {
          // AICODE-NOTE: For now we pass params as JSON in title for backward compatibility
          // TODO: Refactor to use proper parameter passing mechanism
          const result = await params.createDocument.execute({
            title: JSON.stringify(videoParams),
            kind: "video",
          });

          console.log("ðŸŽ¬ âœ… CREATE DOCUMENT RESULT:", result);

          return {
            ...result,
            message: `I'm creating ${operationType.replace("-", " ")} with description: "${prompt}". Using model "${selectedModel.name}" with ${selectedResolution.label} resolution and ${selectedDuration.label} duration. Artifact created and generation started.`,
          };
        } catch (error) {
          console.error("ðŸŽ¬ âŒ CREATE DOCUMENT ERROR:", error);
          throw error;
        }
      } catch (error: any) {
        console.error("ðŸŽ¬ âŒ ERROR CREATING VIDEO DOCUMENT:", error);

        // Create error artifact for better user feedback
        if (params?.createDocument) {
          try {
            const errorResult = await params.createDocument.execute({
              title: JSON.stringify({
                prompt,
                status: "error",
                error: error.message || "Failed to create video document",
                timestamp: Date.now(),
                message: "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾",
              }),
              kind: "video",
            });

            return {
              ...errorResult,
              error: `ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾: ${error.message}`,
              message: `Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾: "${prompt}". ÐžÑˆÐ¸Ð±ÐºÐ°: ${error.message}`,
            };
          } catch (artifactError) {
            console.error(
              "ðŸŽ¬ âŒ Failed to create error artifact:",
              artifactError
            );
          }
        }

        return {
          error: `ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾: ${error.message}`,
          message: `Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾: "${prompt}". ÐžÑˆÐ¸Ð±ÐºÐ°: ${error.message}`,
          fallbackConfig: config,
        };
      }
    },
  });

// Helper function to find video style (similar to image style finder)
export function findVideoStyle(
  styleName: string,
  availableStyles: MediaOption[]
): MediaOption | null {
  const normalizedStyleName = styleName.toLowerCase().trim();

  // Direct match by label or id
  let foundStyle = availableStyles.find(
    (style) =>
      style.label.toLowerCase() === normalizedStyleName ||
      style.id.toLowerCase() === normalizedStyleName
  );

  if (foundStyle) return foundStyle;

  // Partial match
  foundStyle = availableStyles.find(
    (style) =>
      style.label.toLowerCase().includes(normalizedStyleName) ||
      style.id.toLowerCase().includes(normalizedStyleName) ||
      normalizedStyleName.includes(style.label.toLowerCase()) ||
      normalizedStyleName.includes(style.id.toLowerCase())
  );

  return foundStyle || null;
}
