import { createDocumentHandler } from "@/lib/artifacts/server";
import { generateImageWithStrategy } from "@turbo-super/api";
import { getSuperduperAIConfig } from "@/lib/config/superduperai";
import { getStyles } from "@/lib/ai/api/get-styles";
import type { MediaOption, MediaResolution } from "@/lib/types/media-settings";
import type { ImageModel } from "@/lib/config/superduperai";
import { getAvailableImageModels } from "@/lib/config/superduperai";
import { selectImageToImageModel } from "@/lib/generation/model-utils";
import { getMessagesByChatId } from "@/lib/db/queries";
import {
  deductOperationBalance,
} from "@/lib/utils/tools-balance";

// Import the same constants as in configure-image-generation
const RESOLUTIONS: MediaResolution[] = [
  {
    width: 1344,
    height: 768,
    label: "1344x768",
    aspectRatio: "16:9",
    qualityType: "hd",
  },
  {
    width: 1920,
    height: 1080,
    label: "1920√ó1080",
    aspectRatio: "16:9",
    qualityType: "full_hd",
  },
  {
    width: 1664,
    height: 1216,
    label: "1664x1216",
    aspectRatio: "4:3",
    qualityType: "full_hd",
  },
  {
    width: 1152,
    height: 896,
    label: "1152x896",
    aspectRatio: "4:3",
    qualityType: "hd",
  },
  {
    width: 1024,
    height: 1024,
    label: "1024x1024",
    aspectRatio: "1:1",
    qualityType: "hd",
  },
  {
    width: 1408,
    height: 1408,
    label: "1408√ó1408",
    aspectRatio: "1:1",
    qualityType: "full_hd",
  },
  {
    width: 1408,
    height: 1760,
    label: "1408√ó1760",
    aspectRatio: "4:5",
    qualityType: "full_hd",
  },
  {
    width: 1024,
    height: 1280,
    label: "1024x1280",
    aspectRatio: "4:5",
    qualityType: "hd",
  },
  {
    width: 1080,
    height: 1920,
    label: "1080√ó1920",
    aspectRatio: "9:16",
    qualityType: "full_hd",
  },
  {
    width: 768,
    height: 1344,
    label: "768x1344",
    aspectRatio: "9:16",
    qualityType: "hd",
  },
];

const SHOT_SIZES: MediaOption[] = [
  {
    id: "extreme_long_shot",
    label: "Extreme Long Shot",
    description: "Shows vast landscapes or cityscapes with tiny subjects",
  },
  {
    id: "long_shot",
    label: "Long Shot",
    description: "Shows full body of subject with surrounding environment",
  },
  {
    id: "medium_shot",
    label: "Medium Shot",
    description: "Shows subject from waist up, good for conversations",
  },
  {
    id: "medium_close_up",
    label: "Medium Close-Up",
    description: "Shows subject from chest up, good for portraits",
  },
  {
    id: "close_up",
    label: "Close-Up",
    description: "Shows a subject's face or a small object in detail",
  },
  {
    id: "extreme_close_up",
    label: "Extreme Close-Up",
    description:
      "Shows extreme detail of a subject, like eyes or small objects",
  },
  {
    id: "two_shot",
    label: "Two-Shot",
    description: "Shows two subjects in frame, good for interactions",
  },
  {
    id: "detail_shot",
    label: "Detail Shot",
    description: "Focuses on a specific object or part of a subject",
  },
];

// AICODE-NOTE: IMAGE_MODELS now loaded dynamically from API via getAvailableImageModels()

export const imageDocumentHandler = createDocumentHandler<"image">({
  kind: "image",
  onCreateDocument: async ({ id: chatId, title, dataStream, session }) => {
    let draftContent = "";
    try {
      // Parse the title to extract image generation parameters
      const params = JSON.parse(title);
      const {
        prompt,
        style = { id: "flux_steampunk", label: "Steampunk" },
        resolution = {
          width: 1024,
          height: 1024,
          label: "1024x1024",
          aspectRatio: "1:1",
          qualityType: "hd",
        },
        model = { id: "flux-dev", label: "Flux Dev" },
        // Use label form for shot size to match API enum expectations
        shotSize = { id: "Medium Shot", label: "Medium Shot" },
        negativePrompt = "",
        seed,
        batchSize,
      } = params;

      // Balance check is now done in AI tools before artifact creation
      // No need for balance validation here as it's already checked

      // Load dynamic models from SuperDuperAI API
      let availableModels: ImageModel[] = [];
      try {
        availableModels = await getAvailableImageModels();
      } catch (error) {
        console.error("üé® ‚ùå Failed to load dynamic models:", error);
        availableModels = await getAvailableImageModels();
      }

      // Get available styles from API
      let availableStyles: MediaOption[] = [];
      try {
        const response = await getStyles();
        if ("error" in response) {
          console.error("üé® ‚ùå FAILED TO GET STYLES:", response.error);
        } else {
          availableStyles = response.items.map((style) => ({
            id: style.name,
            label: style.title ?? style.name,
          }));
        }
      } catch (err) {
        console.error("üé® ‚ùå ERROR GETTING STYLES:", err);
      }

      // Resolve source image URL (support attachment:// and missing param via chat history)
      const config = getSuperduperAIConfig();
      let effectiveSourceImageUrl: string | undefined = params.sourceImageUrl;

      console.log("üîç Image artifact: resolving source image URL:", {
        sourceImageUrl: params.sourceImageUrl,
        chatId,
        needsResolve:
          !effectiveSourceImageUrl ||
          effectiveSourceImageUrl.startsWith("attachment://"),
      });

      try {
        const needsResolve =
          !effectiveSourceImageUrl ||
          effectiveSourceImageUrl.startsWith("attachment://");
        if (needsResolve) {
          const history = await getMessagesByChatId({ id: chatId });
          console.log("üîç Image artifact: searching chat history:", {
            historyLength: history.length,
          });

          for (let i = history.length - 1; i >= 0; i--) {
            const m = history[i] as any;
            console.log("üîç Image artifact: checking message:", {
              role: m.role,
              hasAttachments: !!m.attachments,
              attachmentsLength: m.attachments?.length || 0,
              attachments: m.attachments,
            });

            if (m.role === "user" && Array.isArray(m.attachments)) {
              const img = m.attachments.find(
                (a: any) =>
                  typeof a?.url === "string" &&
                  /^https?:\/\//.test(a.url) &&
                  String(a?.contentType || "").startsWith("image/")
              );
              if (img?.url) {
                effectiveSourceImageUrl = img.url;
                console.log(
                  "üîç Image artifact: found source image in history:",
                  img.url
                );
                break;
              }
            }
          }
        }
      } catch (error) {
        console.error(
          "üîç Image artifact: error searching chat history:",
          error
        );
      }

      // Decide generation type based on resolved source URL
      const generationType = effectiveSourceImageUrl
        ? "image-to-image"
        : "text-to-image";

      console.log("üîç Image artifact: generation type determined:", {
        effectiveSourceImageUrl,
        generationType,
      });

      // If image-to-image, create File object from URL instead of uploading
      let sourceFile: File | undefined = undefined;
      if (
        generationType === "image-to-image" &&
        effectiveSourceImageUrl &&
        /^https?:\/\//.test(effectiveSourceImageUrl)
      ) {
        try {
          const resp = await fetch(effectiveSourceImageUrl);
          if (!resp.ok)
            throw new Error(`Failed to fetch source image: ${resp.status}`);
          const blob = await resp.blob();
          const filename =
            effectiveSourceImageUrl.split("/").pop() || "source-image.jpg";
          sourceFile = new File([blob], filename, { type: blob.type });
          console.log("üîç Image artifact: created File object from URL:", {
            filename,
            size: sourceFile.size,
            type: sourceFile.type,
          });
        } catch (e) {
          console.error(
            "üé® ‚ùå Failed to create File from source image URL:",
            e
          );
        }
      }

      // Remap model for image-to-image if needed (e.g., inpainting variant)
      let modelForGeneration: any = model;
      if (generationType === "image-to-image") {
        try {
          const rawName = (model as any)?.name || (model as any)?.id || "";
          const availableModels = await getAvailableImageModels();
          console.log("üîç Image artifact: remapping model for img2img:", {
            originalModel: rawName,
            availableModels: availableModels.map((m) => m.name),
          });

          const mapped = await selectImageToImageModel(
            rawName,
            getAvailableImageModels,
            { allowInpainting: true }
          );

          // Force fal-ai/flux-pro/kontext for img2img as it's proven to work
          const kontextModel = availableModels.find(
            (m) => m.name === "fal-ai/flux-pro/kontext"
          );

          if (kontextModel) {
            modelForGeneration = {
              ...(model as any),
              name: kontextModel.name,
            };
            console.log(
              "üîç Image artifact: FORCED kontext model for img2img:",
              kontextModel.name
            );
          } else {
            // Fallback to mapped model if kontext not available
            if (mapped) {
              modelForGeneration = { ...(model as any), name: mapped };
              console.log(
                "üîç Image artifact: fallback to mapped model:",
                mapped
              );
            } else {
              console.log(
                "üîç Image artifact: kontext model not found, using original:",
                rawName
              );
            }
          }
        } catch (error) {
          console.error("üîç Image artifact: error remapping model:", error);
        }
      }
      // For image-to-image: require sourceFile and send minimal payload (no style/resolution/shotSize)
      if (generationType === "image-to-image" && !sourceFile) {
        draftContent = JSON.stringify({
          status: "failed",
          error:
            "Failed to get source image for image-to-image (attachment did not resolve to URL)",
          prompt,
        });
        return draftContent;
      }

      const generationParams: any = {
        prompt,
        model: modelForGeneration,
        negativePrompt,
        seed,
        batchSize,
      };
      if (generationType === "image-to-image") {
        generationParams.file = sourceFile;
        if (
          effectiveSourceImageUrl &&
          /^https?:\/\//.test(effectiveSourceImageUrl)
        ) {
          generationParams.sourceImageUrl = effectiveSourceImageUrl;
        }
        // Add default resolution for img2img to prevent NaN values in SDK
        // Use 1024x1024 as it's proven to work with fal-ai/flux-pro/kontext
        generationParams.resolution = {
          width: 1024,
          height: 1024,
        };
      } else {
        generationParams.style = style;
        generationParams.resolution = resolution;
        generationParams.shotSize = shotSize;
      }

      console.log(
        "üîç Image artifact: calling generateImageWithStrategy with params:",
        {
          generationType,
          generationParams,
          config: {
            url: config.url,
            hasToken: !!config.token,
          },
        }
      );

      const result = await generateImageWithStrategy(
        generationType,
        generationParams,
        config
      );

      console.log("üîç Image artifact: generateImageWithStrategy result:", {
        success: result.success,
        projectId: result.projectId,
        requestId: result.requestId,
        fileId: result.fileId,
        error: result.error,
      });

      // AICODE-DEBUG: API payload removed to reduce duplication
      // If needed for debugging, can be reconstructed from stored parameters

      if (!result.success) {
        draftContent = JSON.stringify({
          status: "failed",
          error: result.error,
          prompt: prompt,
        });
        return draftContent;
      }

      // –§–æ—Ä–º–∏—Ä—É–µ–º content —Ç–æ–ª—å–∫–æ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–±–µ–∑ available –æ–ø—Ü–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
      draftContent = JSON.stringify({
        status: "pending",
        projectId: result.projectId || chatId,
        requestId: result.requestId,
        fileId: result.fileId,
        prompt: prompt,
        // Store only selected values, not all available options
        style,
        resolution,
        model,
        shotSize,
        negativePrompt,
        seed,
        batchSize,
        timestamp: Date.now(),
        message:
          result.message ||
          "Image generation started, connecting to WebSocket...",
      });

      // Deduct balance after successful generation start
      if (session?.user?.id) {
        try {
          const operationType = generationType;
          const multipliers: string[] = [];

          // Check style for quality multipliers
          if (style?.id?.includes("high-quality"))
            multipliers.push("high-quality");
          if (style?.id?.includes("ultra-quality"))
            multipliers.push("ultra-quality");

          await deductOperationBalance(
            session.user.id,
            "image-generation",
            operationType,
            multipliers,
            {
              projectId: result.projectId,
              fileId: result.fileId,
              prompt: prompt.substring(0, 100),
              operationType,
              timestamp: new Date().toISOString(),
            }
          );
          console.log(
            `üí≥ Balance deducted for user ${session.user.id} after image generation start`
          );
        } catch (balanceError) {
          console.error(
            "‚ö†Ô∏è Failed to deduct balance after image generation:",
            balanceError
          );
          // Continue - image generation already started
        }
      }
    } catch (error: any) {
      console.error("üé® ‚ùå IMAGE GENERATION ERROR:", error);
      draftContent = JSON.stringify({
        status: "failed",
        error: error?.message || "Failed to parse image parameters",
      });
    }
    return draftContent;
  },
  onUpdateDocument: async ({ document, description, dataStream }) => {
    let draftContent = document.content;
    try {
      // Check if document already has completed content - don't recreate if so
      if (draftContent) {
        try {
          const existingContent = JSON.parse(draftContent);
          if (
            existingContent.status === "completed" &&
            existingContent.imageUrl
          ) {
            console.log(
              "üé® ‚ö†Ô∏è Document already completed with image, skipping update to prevent reset"
            );
            return draftContent; // Return existing content without recreating
          }
        } catch (parseError) {
          // If we can't parse existing content, proceed with update
          console.log(
            "üé® ‚ÑπÔ∏è Could not parse existing content, proceeding with update"
          );
        }
      }
      // Extract chatId from document.id (which should be the chat ID)
      const chatId = document.id;
      // Parse the description to extract new image generation parameters
      const params = JSON.parse(description);
      const {
        prompt,
        style = { id: "flux_steampunk", label: "Steampunk" },
        resolution = {
          width: 1024,
          height: 1024,
          label: "1024x1024",
          aspectRatio: "1:1",
          qualityType: "hd",
        },
        model = { id: "flux-dev", label: "Flux Dev" },
        // Use label form for shot size to match API enum expectations
        shotSize = { id: "Medium Shot", label: "Medium Shot" },
        negativePrompt = "",
        seed,
        batchSize,
      } = params;
      // Start image generation using new architecture
      // NOTE: onUpdateDocument doesn't have access to session, so using system token fallback
      const config = getSuperduperAIConfig();
      // Resolve source image URL again for update flow
      let effectiveSourceImageUrl: string | undefined = params.sourceImageUrl;
      try {
        const needsResolve =
          !effectiveSourceImageUrl ||
          effectiveSourceImageUrl.startsWith("attachment://");
        if (needsResolve) {
          const history = await getMessagesByChatId({ id: chatId });
          for (let i = history.length - 1; i >= 0; i--) {
            const m = history[i] as any;
            if (m.role === "user" && Array.isArray(m.attachments)) {
              const img = m.attachments.find(
                (a: any) =>
                  typeof a?.url === "string" &&
                  /^https?:\/\//.test(a.url) &&
                  String(a?.contentType || "").startsWith("image/")
              );
              if (img?.url) {
                effectiveSourceImageUrl = img.url;
                break;
              }
            }
          }
        }
      } catch (_) {}

      const generationType = effectiveSourceImageUrl
        ? "image-to-image"
        : "text-to-image";

      // If img2img with source URL, create File object instead of uploading
      let sourceFile: File | undefined = undefined;
      if (
        generationType === "image-to-image" &&
        effectiveSourceImageUrl &&
        /^https?:\/\//.test(effectiveSourceImageUrl)
      ) {
        try {
          const resp = await fetch(effectiveSourceImageUrl);
          if (!resp.ok)
            throw new Error(`Failed to fetch source image: ${resp.status}`);
          const blob = await resp.blob();
          const filename =
            effectiveSourceImageUrl.split("/").pop() || "source-image.jpg";
          sourceFile = new File([blob], filename, { type: blob.type });
          console.log(
            "üîç Image artifact update: created File object from URL:",
            {
              filename,
              size: sourceFile.size,
              type: sourceFile.type,
            }
          );
        } catch (e) {
          console.error(
            "üé® ‚ùå Failed to create File from source image URL:",
            e
          );
        }
      }

      // Remap model for image-to-image
      let modelForUpdate: any = model;
      if (generationType === "image-to-image") {
        try {
          const rawName = (model as any)?.name || (model as any)?.id || "";
          const mapped = await selectImageToImageModel(
            rawName,
            getAvailableImageModels,
            { allowInpainting: true }
          );
          if (mapped) {
            modelForUpdate = { ...(model as any), name: mapped };
          }
        } catch (_) {}
      }
      if (generationType === "image-to-image" && !sourceFile) {
        draftContent = JSON.stringify({
          status: "failed",
          error:
            "Failed to get source image for image-to-image (attachment did not resolve to URL)",
          prompt,
        });
        return draftContent;
      }

      const updateParams: any = {
        prompt,
        model: modelForUpdate,
        negativePrompt,
        seed,
        batchSize,
      };
      if (generationType === "image-to-image") {
        updateParams.file = sourceFile;
        if (
          effectiveSourceImageUrl &&
          /^https?:\/\//.test(effectiveSourceImageUrl)
        ) {
          updateParams.sourceImageUrl = effectiveSourceImageUrl;
        }
      } else {
        updateParams.style = style;
        updateParams.resolution = resolution;
        updateParams.shotSize = shotSize;
      }

      const result = await generateImageWithStrategy(
        generationType,
        updateParams,
        config
      );
      if (!result.success) {
        draftContent = JSON.stringify({
          status: "failed",
          error: result.error,
          prompt: prompt,
        });
        return draftContent;
      }
      draftContent = JSON.stringify({
        status: "pending",
        projectId: result.projectId || chatId,
        requestId: result.requestId,
        fileId: result.fileId,
        prompt: prompt,
        // Store only selected values, not all available options
        style,
        resolution,
        model,
        shotSize,
        negativePrompt,
        seed,
        batchSize,
        timestamp: Date.now(),
        message:
          result.message ||
          "Image generation started, connecting to WebSocket...",
      });
    } catch (error: any) {
      console.error("üé® ‚ùå IMAGE GENERATION ERROR:", error);
      draftContent = JSON.stringify({
        status: "failed",
        error: error?.message || "Failed to update image parameters",
      });
    }
    return draftContent;
  },
});
